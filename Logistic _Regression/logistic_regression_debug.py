from __future__ import division, print_function, unicode_literals
import numpy as np 
import matplotlib.pyplot as plt
np.random.seed(22)

def sigmoid(s):
    return 1/(1 + np.exp(-s))

def logistic_sigmoid_regression_debug(X, y, w_init, eta, tol = 1e-4, max_count = 10000):
    w = [w_init]
    N = X.shape[1]
    d = X.shape[0]
    count = 0
    check_w_after = 20
    debug_limit = 50  # S·ªë l·∫ßn in chi ti·∫øt ƒë·∫ßu ti√™n
    print(f"\n=== B·∫ÆT ƒê·∫¶U HU·∫§N LUY·ªÜN LOGISTIC REGRESSION ===")
    print(f"N = {N} (s·ªë m·∫´u), d = {d} (s·ªë chi·ªÅu)")
    print(f"w_init: {w_init.flatten()}")
    iteration = 0
    while count < max_count:
        iteration += 1
        print(f"\n--- L·∫¶N L·∫∂P {iteration} ---")
        mix_id = np.random.permutation(N)
        print(f"Th·ª© t·ª± x·ª≠ l√Ω m·∫´u: {mix_id}")
        for i in mix_id:
            xi = X[:, i].reshape(d, 1)
            yi = y[i]
            zi = sigmoid(np.dot(w[-1].T, xi))[0, 0]
            w_before = w[-1].flatten()
            w_new = w[-1] + eta*(yi - zi)*xi
            w_after = w_new.flatten()
            count += 1
            # Loss (log-loss) cho m·∫´u n√†y
            loss = - (yi * np.log(zi + 1e-8) + (1 - yi) * np.log(1 - zi + 1e-8))
            if count <= debug_limit:
                print(f"M·∫´u {i}: xi = {xi.flatten()}, yi = {yi}")
                print(f"  D·ª± ƒëo√°n sigmoid: {zi:.4f}")
                print(f"  Loss: {float(loss):.6f}")
                print(f"  w tr∆∞·ªõc c·∫≠p nh·∫≠t: {w_before}")
                print(f"  w sau c·∫≠p nh·∫≠t:   {w_after}")
                if zi > 0.8:
                    print("  ‚Üí Model r·∫•t t·ª± tin l√† l·ªõp 1 (sigmoid g·∫ßn 1)")
                elif zi < 0.2:
                    print("  ‚Üí Model r·∫•t t·ª± tin l√† l·ªõp 0 (sigmoid g·∫ßn 0)")
                else:
                    print("  ‚Üí Model ch∆∞a ch·∫Øc ch·∫Øn (sigmoid ‚âà 0.5)")
            elif count == debug_limit + 1:
                print(f"... (ƒê√£ v∆∞·ª£t qu√° {debug_limit} l·∫ßn c·∫≠p nh·∫≠t, d·ª´ng in chi ti·∫øt) ...")
            if count % check_w_after == 0:
                norm_diff = np.linalg.norm(w_new - w[-check_w_after])
                print(f"  Ki·ªÉm tra h·ªôi t·ª• sau {check_w_after} l·∫ßn c·∫≠p nh·∫≠t: ||w_new - w_old|| = {norm_diff:.6f}")
                if norm_diff < tol:
                    print(f"\nüéâ H·ªòI T·ª§ SAU {count} L·∫¶N C·∫¨P NH·∫¨T!")
                    return w
            w.append(w_new)
    print(f"\n‚ö†Ô∏è ƒê·∫†T GI·ªöI H·∫†N S·ªê L·∫¶N L·∫∂P (max_count = {max_count})")
    return w

# ====== T·∫†O D·ªÆ LI·ªÜU ======
means = [[2, 2], [4, 2]]
cov = [[.7, 0], [0, .7]]
N = 20
X0 = np.random.multivariate_normal(means[0], cov, N)
X1 = np.random.multivariate_normal(means[1], cov, N)

print("\n=== TH√îNG TIN D·ªÆ LI·ªÜU ===")
print(f"X0 shape: {X0.shape}, X1 shape: {X1.shape}")
print(f"X0 (5 m·∫´u ƒë·∫ßu):\n{X0[:5]}")
print(f"X1 (5 m·∫´u ƒë·∫ßu):\n{X1[:5]}")

X = np.concatenate((X0, X1), axis = 0).T
print(f"\nX shape sau khi gh√©p: {X.shape}")

y = np.concatenate((np.zeros((1, N)), np.ones((1, N))), axis = 1).T
print(f"y shape: {y.shape}")
print(f"y (10 ph·∫ßn t·ª≠ ƒë·∫ßu): {y[:10].flatten()}")

# Xbar 
X = np.concatenate((np.ones((1, 2*N)), X), axis = 0)
print(f"X shape sau khi th√™m bias: {X.shape}")
print(f"X (5 c·ªôt ƒë·∫ßu):\n{X[:, :5]}")

eta = .05 
d = X.shape[0]
w_init = np.random.randn(d, 1)

w = logistic_sigmoid_regression_debug(X, y, w_init, eta, tol = 1e-4, max_count= 10000)
print(f"\n=== K·∫æT QU·∫¢ CU·ªêI C√ôNG ===")
print(f"w cu·ªëi c√πng: {w[-1].flatten()}")
print(f"S·ªë l·∫ßn c·∫≠p nh·∫≠t: {len(w)-1}") 