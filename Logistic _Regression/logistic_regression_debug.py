from __future__ import division, print_function, unicode_literals
import numpy as np 
import matplotlib.pyplot as plt
np.random.seed(22)

def sigmoid(s):
    return 1/(1 + np.exp(-s))

def logistic_sigmoid_regression_debug(X, y, w_init, eta, tol = 1e-4, max_count = 10000):
    w = [w_init]
    N = X.shape[1]
    d = X.shape[0]
    count = 0
    check_w_after = 20
    debug_limit = 50  # Sá»‘ láº§n in chi tiáº¿t Ä‘áº§u tiÃªn
    print(f"\n=== Báº®T Äáº¦U HUáº¤N LUYá»†N LOGISTIC REGRESSION ===")
    print(f"N = {N} (sá»‘ máº«u), d = {d} (sá»‘ chiá»u)")
    print(f"w_init: {w_init.flatten()}")
    while count < max_count:
        mix_id = np.random.permutation(N)
        print(f"\n--- Láº¦N Láº¶P {(count//N)+1} ---")
        print(f"Thá»© tá»± xá»­ lÃ½ máº«u: {mix_id}")
        for i in mix_id:
            xi = X[:, i].reshape(d, 1)
            yi = y[i]
            zi = sigmoid(np.dot(w[-1].T, xi))[0, 0]
            w_before = w[-1].flatten()
            w_new = w[-1] + eta*(yi - zi)*xi
            w_after = w_new.flatten()
            count += 1
            # Loss (log-loss) cho máº«u nÃ y
            loss = - (yi * np.log(zi + 1e-8) + (1 - yi) * np.log(1 - zi + 1e-8))
            if count <= debug_limit:
                print(f"Máº«u {i}: xi = {xi.flatten()}, yi = {yi}")
                print(f"  Dá»± Ä‘oÃ¡n sigmoid: {zi:.4f}")
                print(f"  Loss: {float(loss):.6f}")
                print(f"  w trÆ°á»›c cáº­p nháº­t: {w_before}")
                print(f"  w sau cáº­p nháº­t:   {w_after}")
            elif count == debug_limit + 1:
                print(f"... (ÄÃ£ vÆ°á»£t quÃ¡ {debug_limit} láº§n cáº­p nháº­t, dá»«ng in chi tiáº¿t) ...")
            if count % check_w_after == 0:
                norm_diff = np.linalg.norm(w_new - w[-check_w_after])
                print(f"  Kiá»ƒm tra há»™i tá»¥ sau {check_w_after} láº§n cáº­p nháº­t: ||w_new - w_old|| = {norm_diff:.6f}")
                if norm_diff < tol:
                    print(f"\nğŸ‰ Há»˜I Tá»¤ SAU {count} Láº¦N Cáº¬P NHáº¬T!")
                    return w
            w.append(w_new)
    print(f"\nâš ï¸ Äáº T GIá»šI Háº N Sá» Láº¦N Láº¶P (max_count = {max_count})")
    return w

# ====== Táº O Dá»® LIá»†U ======
means = [[2, 2], [4, 2]]
cov = [[.7, 0], [0, .7]]
N = 20
X0 = np.random.multivariate_normal(means[0], cov, N)
X1 = np.random.multivariate_normal(means[1], cov, N)

print("\n=== THÃ”NG TIN Dá»® LIá»†U ===")
print(f"X0 shape: {X0.shape}, X1 shape: {X1.shape}")
print(f"X0 (5 máº«u Ä‘áº§u):\n{X0[:5]}")
print(f"X1 (5 máº«u Ä‘áº§u):\n{X1[:5]}")

X = np.concatenate((X0, X1), axis = 0).T
print(f"\nX shape sau khi ghÃ©p: {X.shape}")

y = np.concatenate((np.zeros((1, N)), np.ones((1, N))), axis = 1).T
print(f"y shape: {y.shape}")
print(f"y (10 pháº§n tá»­ Ä‘áº§u): {y[:10].flatten()}")

# Xbar 
X = np.concatenate((np.ones((1, 2*N)), X), axis = 0)
print(f"X shape sau khi thÃªm bias: {X.shape}")
print(f"X (5 cá»™t Ä‘áº§u):\n{X[:, :5]}")

eta = .05 
d = X.shape[0]
w_init = np.random.randn(d, 1)

w = logistic_sigmoid_regression_debug(X, y, w_init, eta, tol = 1e-4, max_count= 10000)
print(f"\n=== Káº¾T QUáº¢ CUá»I CÃ™NG ===")
print(f"w cuá»‘i cÃ¹ng: {w[-1].flatten()}")
print(f"Sá»‘ láº§n cáº­p nháº­t: {len(w)-1}") 